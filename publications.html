---
layout: diy
title: Publications
---

<head>
<style>
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #6e6f71; }
p { font-size : 16px; }
h3 { font-size : 18px; margin : 8; padding : 0; }
.container { width : 1000px;}
.publogo { width: 100 px; margin-right : 20px; float : left; border : 10px;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 180px; padding-top : 0px;}
.publication strong { font-size : 17px; color : #990036; }
.publication strong a { font-size : 17px; color : #990036; }
</style>
</head>

<div class="container">

<font color="grey" size="3">
  * Equal contribution. ♡ Project lead. ✉ corresponding / co-corresponding author.
</font>


<h3>2024</h3>

<div class="publication">
  <img src="../static/pubs/SCW23.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    MovieChat: From Dense Token to Sparse Memory in Long Video Understanding
    </strong>
    <br>
    Enxin Song*, Wenhao Chai*♡, <b>Guanhong Wang*</b>,Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Tian Ye, Jenq-Neng Hwang, Gaoang Wang✉
    <br>
    <font color=#E89B00>
    <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024</em>
    </font>
    <br>
    <a href="https://rese1f.github.io/MovieChat/">[Website]</a>
    <a href="https://arxiv.org/abs/2307.16449">[Paper]</a>
    <a href="https://github.com/rese1f/MovieChat">[Dataset]</a>
    <a href="https://github.com/rese1f/MovieChat">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
    <br>
    <font color="grey" size="2">
    MovieChat achieves state-of-the-art performace in long video understanding by introducing memory mechanism.
    </font>
  </p>
</div>

<div class="publication">
  <img src="../static/pubs/LGGCD24.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    Solving the Catastrophic Forgetting Problem in Generalized Category Discovery
    </strong>
    <br>
    Xinzi Cao, Xiawu Zheng, <b>Guanhong Wang</b>, Weijiang Yu, Yunhang Shen, Ke Li, Yutong Lu✉, Yonghong Tian✉
    <br>
    <font color=#E89B00>
    <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024</em>
    </font>
<!--     <br>
    <a href="https://rese1f.github.io/MovieChat/">[Website]</a>
    <a href="https://arxiv.org/abs/2307.16449">[Paper]</a>
    <a href="https://github.com/rese1f/MovieChat">[Dataset]</a>
    <a href="https://github.com/rese1f/MovieChat">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
    <br> -->
    <br>
    <font color="grey" size="2">
    We propose LegoGCD, which is seamlessly integrated into previous methods to enhance the discrimination of novel classes while
    maintaining performance on previously encountered known classes.
    </font>
  </p>
</div>


<br> 
<h3>2023</h3>
 <br> 
<div class="publication">
  <img src="../static/pubs/KPF23.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    Knowledge-guided Pre-training and Fine-tuning: Video Representation Learning for Action Recognition
    </strong>
    <br>
    <b>Guanhong Wang*</b>, Yang Zhou, Zhanhao He, Keyu Lu, Yang Feng, Zuozhu Liu, Gaoang Wang✉
    <br>
    <font color=#E89B00>
    <em>Neurocomputing, 2023</em>
    </font>
    <br>
    <font color="grey" size="2">
    We propose a novel video representation learning method with knowledge-guided pre-training and fine-tuning for action recognition.
    </font>
  </p>
</div>
<br> 
 
<div class="publication">
  <img src="../static/pubs/WWC23.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning
    </strong>
    <br>
    Xuan Wang*, <b>Guanhong Wang*</b>, Wenhao Chai, Jiayu Zhou, Gaoang Wang✉
    <br>
    <font color=#E89B00>
    <em>Chinese Conference on Pattern Recognition and Computer Vision (PRCV), 2023</em>
    </font>
    <br>
    <font color="grey" size="2">
    Personalized image captioning incorporate user prior knowledge into the model, such as writing styles and preferred vocabularies
    </font>
  </p>
</div>

 
<br>

<div class="publication">
  <img src="../static/pubs/ZCH23.png" class="publogo" width="200 px" height="180 px">
  <p> 
    <strong>
    A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision
    </strong>
    <br>
    Zhonghan Zhao*, Wenhao Chai*, Shengyu Hao, Wenhao Hu, <b>Guanhong Wang</b>, Shidong Cao, Gaoang Wang✉, Mingli Song, Jenq-Neng Hwang
    <br>
    <em>arXiv Preprint.</em>
    <br>
    <a href="https://arxiv.org/abs/2307.03353">[Paper]</a>
    <br>
    <font color="grey" size="2">
    Our survey provides valuable reference material for researchers interested in deep learning applications within the sporting industry whilst also shedding light on its potential to utilize sports data for analysis.
    </font>
  </p>
</div>


<br>

<h3>2022</h3>

<div class="publication">
  <img src="../static/pubs/M3S22.png" class="publogo" width="200 px" height="200 px">
  <p> 
    <strong>
    Missing Modality meets Meta Sampling (M3S): An Efficient Universal Approach for Multimodal Sentiment Analysis with Missing Modality
    </strong>
    <br>
    Haozhe Chi*, Minghua Yang, Junhao Zhu, <b>Guanhong Wang</b>, Gaoang Wang✉
    <br>
    <font color=#E89B00>
    <em>Asia-Pacific Chapter of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing (AACL-IJCNLP), 2022</em> 
    </font>
    <a href="arXiv preprint arXiv:2210.03428">[Paper]</a>
    <br>
    <font color="grey" size="2">
    In this paper, we propose a simple yet effective meta-sampling approach for multimodal sentiment analysis with missing modalities, namely Missing Modality-based Meta Sampling (M3S).
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/ARPT22.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    Human-centered Prior-guided and Task-dependent Multi-task Representation Learning for Action Recognition Pre-training
    </strong>
    <br>
    <b>Guanhong Wang*</b>, Keyu Lu, Yang Zhou, Zhanhao He, Gaoang Wang✉ 
    <br>
    <font color="#E89B00">
    <em>IEEE International Conference on Multimedia and Expo (ICME), 2022</em>
    </font>
    <br>
    <a href="https://arxiv.org/pdf/2204.12729">[Paper]</a>
    <br>
    <font color="grey" size="2">
    We distill knowledge from a human parsing model to enrich the semantic capability of representation.
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/MFRN22.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    Multi-feature fusion refine network for video captioning
    </strong>
    <br>
    <b>Guanhong Wang</b>, Hongbo Zhang, Jixiang Du✉
    <br>
    <font color="#E89B00">
    <em>Journal of Experimental & Theoretical Artificial Intelligence (JETAI), 2022</em>
    </font>
    <br>
    <a href="https://www.tandfonline.com/doi/abs/10.1080/0952813X.2021.1883745">[Paper]</a>
    <br>
    <font color="grey" size="2">
    In this paper, we propose an approach based on multi-feature fusion refine network.
    </font>
  </p>
</div>

</div>
