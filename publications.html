---
layout: diy
title: Publications
---

<head>
<style>
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #6e6f71; }
p { font-size : 16px; }
h3 { font-size : 18px; margin : 8; padding : 0; }
.container { width : 1000px;}
.publogo { width: 100 px; margin-right : 20px; float : left; border : 10px;}
.publication { clear : left; padding-bottom : 0px; }
.publication p { height : 180px; padding-top : 0px;}
.publication strong { font-size : 17px; color : #990036; }
.publication strong a { font-size : 17px; color : #990036; }
</style>
</head>

<div class="container">

<!-- <a href="https://scholar.google.com/citations?user=SL--7UMAAAAJ&hl=en" target="_blank">Google Scholar</a>
<br> -->
<font color="grey" size="3">
  * Equal contribution. ♡ Project lead. ✉ corresponding / co-corresponding author.
</font>


<h3>2023</h3>

<div class="publication">
  <img src="../static/pubs/SCW23.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    MovieChat: From Dense Token to Sparse Memory in Long Video Understanding
    </strong>
    <br>
    Enxin Song*, Wenhao Chai*♡, <b>Guanhong Wang*</b>,Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Tian Ye, Jenq-Neng Hwang, Gaoang Wang✉
    <br>
    <em>arXiv Preprint.</em>
    <br>
    <a href="https://rese1f.github.io/MovieChat/">[Website]</a>
    <a href="https://arxiv.org/abs/2307.16449">[Paper]</a>
    <a href="https://github.com/rese1f/MovieChat">[Dataset]</a>
    <a href="https://github.com/rese1f/MovieChat">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
    <br>
    <font color="grey" size="2">
    MovieChat achieves state-of-the-art performace in long video understanding by introducing memory mechanism.
    </font>
  </p>
</div>

<!-- <br>

<div class="publication">
  <img src="../static/pubs/SAL23.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    Sequential Affinity Learning for Video Restoration
    </strong>
    <br>
    Tian Ye, Sixiang Chen, Yun Liu, <b>Wenhao Chai</b>, Jinbin Bai, Wenbin Zou, Yunchen Zhang, Jiang Mingchao, Erkang Chen✉, Chenghao Xue  
    <br>
    <font color="#E89B00">
    <em>ACM Multimedia (ACM MM), 2023</em>
    </font>
    <br>
    <a href="https://github.com/Owen718/SALN">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/Owen718/SALN?style=social">
    <br>
    <font color="grey" size="2">
    Affinity mechanism establishes direct correspondences between the Query frame, degraded sequence, and restored frames in latent space.
    </font>
  </p>
</div> -->

<!-- <br>

<div class="publication">
  <img src="../static/pubs/PMP23.png" class="publogo" width="200 px" height="150 px">
  <p> 
    <strong>
    PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Enhanced 3D Human Pose Estimation
    </strong>
    <br>
    Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, <b>Wenhao Chai</b>, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie✉ 
    <br>
    <font color="#E89B00">
    <em>ACM Multimedia (ACM MM), 2023</em>
    </font>
    <br>
    <font color="grey" size="2">
    PoSynDA offers a state-of-the-art domain adaptation solution for 3D pose estimation.
    </font>
  </p>
</div>

<br> -->

<!-- <div class="publication">
  <img src="../static/pubs/IRF23.png" class="publogo" width="200 px" height="180 px">
  <p> 
    <strong>
    Image Reference-guided Fashion Design with Structure-aware Transfer by Diffusion Models
    </strong>
    <br>
    Shidong Cao*, <b>Wenhao Chai*</b>, Shengyu Hao, Gaoang Wang✉
    <br>
    <font color="#E89B00">
    <em>Computer Vision and Pattern Recognition Workshop (CVPRW), 2023</em>
    </font>
    <br>
    <a href="https://openaccess.thecvf.com/content/CVPR2023W/CVFAD/html/Cao_Image_Reference-Guided_Fashion_Design_With_Structure-Aware_Transfer_by_Diffusion_Models_CVPRW_2023_paper.html">[Paper]</a>
    <a href="https://github.com/Rem105-210/DiffFashion">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/Rem105-210/DiffFashion?style=social">
    <br>
    <font color="grey" size="2">
    we aim to transfer a reference appearance image onto a clothing image while preserving the structure of the clothing image.
    </font>
  </p>
</div>

<br> -->

<!-- <div class="publication">
  <img src="../static/pubs/STC23.png" class="publogo" width="200 px" height="150 px">
  <p> 
    <strong>
    StableVideo: Text-driven Consistency-aware Diffusion Video Editing
    </strong>
    <br>
    <b>Wenhao Chai</b>, Xun Guo, Gaoang Wang, Yan Lu✉
    <br>
    <font color="#E89B00">
    <em>International Conference on Computer Vision (ICCV), 2023</em>
    </font>
    <br>
    <font color="grey" size="2">
    We tackle introduce temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the new objects.
    </font>
  </p>
</div>

<br> -->

<!-- <div class="publication">
  <img src="../static/pubs/GAM23.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation
    </strong>
    <br>
    <b>Wenhao Chai</b>, Zhongyu Jiang, Jenq-Neng Hwang, Gaoang Wang✉
    <br>
    <font color="#E89B00">
    <em>International Conference on Computer Vision (ICCV), 2023</em>
    </font>
    <br>
    <a href="https://arxiv.org/abs/2303.16456">[Paper]</a>
    <a href="https://github.com/rese1f/PoseDA">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/PoseDA?style=social">
    <br>
    <font color="grey" size="2">
    A simple yet effective framework of unsupervised domain adaptation for 3D human pose estimation.
    </font>
  </p>
</div>

<br> -->

<!-- <div class="publication">
  <img src="../static/pubs/DLM23.png" class="publogo" width="200 px" height="180 px">
  <p> 
    <strong>
    Deep Learning Methods for Small Molecule Drug Discovery: A Survey
    </strong>
    <br>
    Wenhao Hu*, Yingying Liu*, Xuanyu Chen, <b>Wenhao Chai</b>, Hangyue Chen, Hongwei Wang✉, Gaoang Wang✉
    <br>
    <font color="#E89B00">
    <em>IEEE Transactions on Artificial Intelligence</em>
    </font>
    <br>
    <a href="https://arxiv.org/abs/2303.00313">[Paper]</a>
    <br>
    <font color="grey" size="2">
    we present a comprehensive review on the aforementioned four aspects, and discuss the relationships among different applications.
    </font>
  </p>
</div>

<br> -->

<!-- <div class="publication">
  <img src="../static/pubs/IRF23.png" class="publogo" width="200 px" height="180 px">
  <p> 
    <strong>
    DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models
    </strong>
    <br>
    Shidong Cao*, <b>Wenhao Chai*</b>, Shengyu Hao, Yanting Zhang, Hangyue Chen✉, Gaoang Wang✉
    <br>
    <font color="#E89B00">
    <em>IEEE Transactions on Multimedia</em>
    </font>
    <br>
    <a href="https://arxiv.org/abs/2302.06826">[Paper]</a>
    <a href="https://github.com/Rem105-210/DiffFashion">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/Rem105-210/DiffFashion?style=social">
    <br>
    <font color="grey" size="2">
    we aim to transfer a reference appearance image onto a clothing image while preserving the structure of the clothing image.
    </font>
  </p>
</div>

<br> -->

<div class="publication">
  <img src="../static/pubs/WWC23.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning
    </strong>
    <br>
    Xuan Wang*, <b>Guanhong Wang*</b>, Wenhao Chai, Jiayu Zhou, Gaoang Wang✉
    <br>
    <em>Under Review.</em>
    <br>
    <font color="grey" size="2">
    Personalized image captioning incorporate user prior knowledge into the model, such as writing styles and preferred vocabularies
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/BIW23.png" class="publogo" width="200 px" height="150 px">
  <p> 
    <strong>
    Blind Inpainting with Object-aware Discrimination for Artificial Marker Removal
    </strong>
    <br>
    Xuechen Guo, Wenhao Hu, Chiming Ni, <b>Wenhao Chai</b>, Shiyan Li, Gaoang Wang✉
    <br>
    <em>arXiv Preprint.</em>
    <br>
    <a href="https://arxiv.org/abs/2303.15124">[Paper]</a>
    <br>
    <font color="grey" size="2">
    A novel blind inpainting network for artificial marker removal in medical images.
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/ZCJ23.png" class="publogo" width="200 px" height="180 px">
  <p> 
    <strong>
    MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling
    </strong>
    <br>
    Zhenyu Zhang*, <b>Wenhao Chai*</b>, Zhongyu Jiang, Tian Ye, Mingli Song, Jenq-Neng Hwang, Gaoang Wang✉
    <br>
    <em>arXiv Preprint.</em>
    <br>
    <a href="https://arxiv.org/abs/2306.17201">[Paper]</a>
    <a href="https://github.com/vvirgooo2/MPM">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/vvirgooo2/MPM?style=social">
    <br>
    <font color="grey" size="2">
    Treat 2D and 3D pose as two different modalities and apply three mask modeling based pretext tasks for human pose pre-training to learn spatial and temporal
relations.
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/JZL23.png" class="publogo" width="200 px" height="180 px">
  <p> 
    <strong>
    Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation
    </strong>
    <br>
    Zhongyu Jiang, Zhuoran Zhou, Lei Li, <b>Wenhao Chai</b>, Cheng-Yen Yang, Jenq-Neng Hwang✉
    <br>
    <em>arXiv Preprint.</em>
    <br>
    <a href="https://arxiv.org/abs/2307.03833">[Paper]</a>
    <a href="https://github.com/ipl-uw/ZeDO-Release">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/ipl-uw/ZeDO-Release?style=social">
    <br>
    <font color="grey" size="2">
    By combining the advantages of optimization-based and learning-based methods, We propose the Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the problem of cross-domain and in-the-wild scenarios.
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/ZCH23.png" class="publogo" width="200 px" height="180 px">
  <p> 
    <strong>
    A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision
    </strong>
    <br>
    Zhonghan Zhao*, <b>Wenhao Chai*</b>, Shengyu Hao, Wenhao Hu, Guanhong Wang, Shidong Cao, Gaoang Wang✉, Mingli Song, Jenq-Neng Hwang
    <br>
    <em>arXiv Preprint.</em>
    <br>
    <a href="https://arxiv.org/abs/2307.03353">[Paper]</a>
    <br>
    <font color="grey" size="2">
    Our survey provides valuable reference material for researchers interested in deep learning applications within the sporting industry whilst also shedding light on its potential to utilize sports data for analysis.
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/UDA23.png" class="publogo" width="200 px" height="150 px">
  <p> 
    <strong>
    Unsupervised Domain Adaptation Approach for Vision-based Semantic Understanding of Bridge Inspection Scenes without Manual Annotations
    </strong>
    <br>
    Yasutaka Narazaki✉, Wendong Pang, Gaoang Wang, <b>Wenhao Chai</b>
    <br>
    <em>Under Review.</em>
    <br>
    <font color="grey" size="2">
    To facilitate the applications of deep learning-based visual recognition algorithms to bridge inspection tasks with limited manual efforts.
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/CCH23.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    Random Bridge Generator as a Platform for Developing Computer Vision-based Structural Inspection Algorithms
    </strong>
    <br>
    Haojia Cheng*, <b>Wenhao Chai*</b>, Jiabao Hu*, Wenhao Ruan*, Mingyu Shi, Hyunjun Kim, Yifan Cao, Yasutaka Narazaki✉
    <br>
    <em>Under Review.</em>
    <br>
    <font color="grey" size="2">
    Random bridge generator uses Blender to generate random-shaped bridges with appropriate random texture. 
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/LAT23.png" class="publogo" width="200 px" height="150 px">
  <p> 
    <strong>
    Learning Action Tokens via Video Diffusion Models
    </strong>
    <br>
    <b>Wenhao Chai</b>, Xun Guo, Gaoang Wang, Yan Lu✉
    <br>
    <em>Under Review.</em>
    <br>
    <font color="grey" size="2">
    We are the first to introduce action token concept in video generation, which can learn actions or changes across time independent from objects.
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/FAN23.png" class="publogo" width="200 px" height="180 px">
  <p> 
      <strong>
      Five A+ Network: You Only Need 9K Parameters for Underwater Image Enhancement
      </strong>
      <br>
      Jingxia Jiang, Tian Ye, Jinbin Bai, Sixiang Chen, <b>Wenhao Chai</b>, Jun Shi, Yun Liu, Erkang Chen✉
      <br>
      <em>arXiv Preprint.</em>
      <br>
      <a href="https://arxiv.org/abs/2305.08824">[Paper]</a>
      <a href="https://github.com/Owen718/FiveAPlus-Network">[Code]</a>
      <img alt="NPM" src="https://img.shields.io/github/stars/Owen718/FiveAPlus-Network?style=social">
      <br>
      <font color="grey" size="2">
      A highly efficient and lightweight real-time underwater image enhancement network with only 9k parameters and 10ms processing time. 
      </font>
  </p>
</div>

<br>

<h3>2022</h3>

<div class="publication">
  <img src="../static/pubs/ASU22.png" class="publogo" width="200 px" height="200 px">
  <p> 
    <strong>
    Automatic Spinal Ultrasound Image Segmentation and Deployment for Real-time Spine Volumetric Reconstruction
    </strong>
    <br>
    Yifan Cao*, Chenghao Tan*, Wenzhuo Qian, <b>Wenhao Chai</b>, Luhang Cui, Wenxuan Yang, Xinben Hu✉, Yongjian Zhu, Wenhui Zhou✉, Xingfa Shen
    <br>
    <font color=#E89B00>
    <em>International Conference on Unmanned Systems (ICUS), 2022</em> 
    </font>
    <br>
    <font color="red">Best Paper</font>
    <br>
    <a href="https://ieeexplore.ieee.org/document/9987127">[Paper]</a>
    <a href="https://github.com/deeper-coder/ICUS-2022">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/deeper-coder/ICUS-2022?style=social">
    <br>
    <font color="grey" size="2">
    A real-time 3D spine volumetric reconstruction from spinal ultrasound images.
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/WST22.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    Weakly Supervised Two-Stage Training Scheme for Deep Video Fight Detection Model
    </strong>
    <br>
    Zhenting Qi*, Ruike Zhu*, Zheyu Fu*, <b>Wenhao Chai*</b>, Volodymyr Kindratenko✉ 
    <br>
    <font color="#E89B00">
    <em>International Conference on Tools with Artificial Intelligence (ICTAI), 2022</em>
    </font>
    <br>
    <a href="https://arxiv.org/abs/2209.11477">[Paper]</a>
    <a href="https://github.com/Hepta-Col/VideoFightDetection">[Dataset]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/Hepta-Col/VideoFightDetection?style=social">
    <br>
    <font color="grey" size="2">
    We collect a new dataset, VFD-2000, that specializes in video fight detection, with a larger scale and more scenarios.
    </font>
  </p>
</div>

<br>

<div class="publication">
  <img src="../static/pubs/DVM22.png" class="publogo" width="200 px" height="160 px">
  <p> 
    <strong>
    Deep Vision Multimodal Learning: Methodology, Benchmark, and Trend
    </strong>
    <br>
    <b>Wenhao Chai</b>, Gaoang Wang✉
    <br>
    <font color="#E89B00">
    <em>Applied Sciences</em>
    </font>
    <br>
    <a href="https://www.mdpi.com/2076-3417/12/13/6588">[Paper]</a>
    <br>
    <font color="grey" size="2">
    With the fast development of deep learning, vision multimodal learning has gained much interest from the community.
    </font>
  </p>
</div>

</div>
